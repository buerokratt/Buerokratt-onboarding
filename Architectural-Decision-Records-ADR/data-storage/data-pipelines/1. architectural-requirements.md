# ARCH-001: Architectural Modularity

## Context

To ensure **scalability, flexibility, and maintainability**, the architecture must be fully modular. Each functionality should operate as a **stand-alone service**, accessed through a well-defined **API**, and be **fully replaceable** without modifying other components. This approach supports seamless upgrades, alternative implementations, and independent scaling.

## Decision

1. **Independent Services**
   - Each functionality (e.g., PDF processing, HTML parsing, web scraping, API-based data fetching, BI tool integration) must be developed as a **separate, self-contained application**.
   - No direct dependencies between modules; all communication must occur via APIs.

2. **API-Driven Design**
   - Each module must expose a **RESTful API** for interaction.
   - APIs must follow a **consistent versioning scheme** to ensure backward compatibility.

3. **Replaceability & Interchangeability**
   - Any component should be replaceable with minimal or no changes to the overall system.
   - Swappable implementations (e.g., using a different HTML parser or OCR engine) must not require changes outside the API integration layer.

4. **Technology-Agnostic Integration**
   - Services should support **polyglot implementations** (i.e., modules can be developed in different languages as long as they adhere to API contracts).
   - External tools (BI, NLP, ETL) must interact with the system using standard protocols (e.g., HTTP, gRPC, WebSockets).

## Example

### Use (Standalone API for PDF Processing)
```json
{
  "service": "pdf_processor",
  "version": "1.0",
  "input": "https://example.com/document.pdf",
  "parameters": { "extract_text": true, "convert_to": "markdown" },
  "output": "https://storage.example.com/results/output.md"
}
```

### Forbidden (Tightly Coupled Processing in a Monolithic System)
```python
# Example of directly embedding PDF processing in a core pipeline
import pdf_processor

def extract_text_from_pdf(file_path):
    return pdf_processor.extract_text(file_path)
```

## Consequences

### **Positive Outcomes**
- **Scalability** – Each service can be deployed, scaled, and optimized independently.
- **Flexibility** – Components can be upgraded or replaced without affecting the entire system.
- **Resilience** – Failures in one module do not impact others.

### **Potential Trade-offs**
- **Increased Complexity** – Managing multiple independent services requires strong API governance.
- **Network Overhead** – API-based interactions introduce latency compared to direct in-memory calls.

---

# ARCH-002: External ETL Integration for NLP Processing

## Context

To ensure **efficient text data ingestion and processing**, the architecture must integrate seamlessly with **NLP-ready ETL tools**. These tools handle **text tokenization, language detection, and metadata extraction**, enabling structured ingestion from **web scrapers, APIs, and document repositories**.

## Decision

1. **Support for NLP-Ready ETL Tools**
   - The architecture must support integration with **widely used NLP frameworks** that provide built-in tokenization, lemmatization, and named entity recognition (NER).
   - Examples of such frameworks include **Spark NLP, spaCy pipelines**, and similar tools.
   - Text processing pipelines must allow for **configurable preprocessing steps**, ensuring adaptability to different NLP workflows.

2. **Event-Driven Text Ingestion**
   - ETL processes must support event-driven ingestion from:
     - **Web scrapers** (e.g., extracting articles, blogs, reports).
     - **API-based data sources** (e.g., government records, financial data feeds).
     - **Document repositories** (e.g., PDFs, HTML pages, TXT files).

3. **Standardized Metadata Extraction**
   - Extracted text must include **language, timestamp, source URL, and document classification**.
   - NLP metadata (e.g., token count, named entities, detected topics) should be structured in JSON format.

4. **Scalability & Fault Tolerance**
   - ETL pipelines must support **horizontal scaling** to handle large text datasets.
   - Automatic retries and failure handling must be implemented to prevent data loss.

## Example

### Use (Event-Driven ETL Ingesting Web-Scraped Data)
```json
{
  "source": "https://example.com/news",
  "timestamp": "2024-03-15T14:00:00Z",
  "language": "en",
  "content": "Breaking news article content...",
  "metadata": {
    "tokenized": true,
    "lemmatized": false,
    "entities_extracted": true
  }
}
```

### Forbidden (Manual Processing Without ETL Automation)
```csv
Timestamp, Source, Content
"2024-03-15", "https://example.com/news", "Breaking news article content..."
```

## Consequences

### **Positive Outcomes**
- **Automated Processing** – Reduces manual effort for text ingestion and preparation.
- **Scalability** – Supports high-volume text data processing.
- **Interoperability** – Ensures compatibility with modern NLP workflows.

### **Potential Trade-offs**
- **ETL Complexity** – Requires maintaining multiple ingestion pipelines.
- **Processing Latency** – Event-driven ingestion may introduce minor delays.

---

# ARCH-003: Event-Driven Data Flow Automation

## Context

To ensure **efficient, scalable, and reliable data movement**, the architecture must incorporate **event-driven data flow automation tools**. These tools manage **ingestion, transformation, and routing of text data** before or after NLP processing. The integration must allow for seamless connectivity with **storage, processing, and visualization components**.

## Decision

1. **Data Flow Automation Requirement**
   - The system must use a dedicated **data flow automation framework** to handle real-time and batch text data movement.
   - Examples of such frameworks include **Apache NiFi**, which provides built-in support for ingesting, transforming, and routing data.

2. **Integration with NLP Pipelines**
   - The automation tool should facilitate passing raw text to **various NLP tools** via API calls.
   - NLP-enriched data should be **routed to storage (e.g., Elasticsearch, S3, databases) or visualization tools**.

3. **Event-Driven Processing**
   - The automation framework must support **trigger-based processing** based on incoming data streams.
   - Both **real-time and batch workflows** must be supported.

## Example

### Use (Automated Ingestion and Routing of API Data)
```json
{
  "source": "https://example.com/api/data",
  "processed_by": "data_flow_automation",
  "destination": "nlp_service",
  "status": "queued"
}
```

### Forbidden (Manual Data Routing Without Automation)
```json
{
  "source": "https://example.com/api/data",
  "processed_by": "manual_script",
  "nlp_processing": true
}
```

## Consequences

### **Positive Outcomes**
- **Efficient Data Routing** – Streamlines text ingestion and distribution.
- **Separation of Concerns** – NLP processing remains independent of data transport.
- **Scalability** – Supports large-scale, real-time text ingestion workflows.

### **Potential Trade-offs**
- **Additional Overhead** – Introducing a dedicated automation tool adds complexity.
- **Dependency on External Services** – Requires integration with NLP and storage systems.
